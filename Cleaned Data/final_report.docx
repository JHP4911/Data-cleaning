What's on the menu?
End-to-End Data Cleaning Workflow

Abstract: - Any data science or data analysis project starts by having an initial screening that lays the foundation of data engineering pipeline that starts by cleaning the data followed by feature engineering followed by feature selection and then at the last we apply some modelling techniques. Data cleaning is an integral part of this system that helps companies save a lot of time and money and helps in boosting the predictive power of a model.
  
1   Introduction
The aim of this report is to generalize the data cleaning pipeline that was followed to have an end to end workflow. We will be using the tools and the concept that we have learnt as a part of the course and apply them to the dataset.
2    Overview and initial assessment
2.1   Origin
The New York Public Library’s menu collection, housed in the Rare Book Division, originated through the energetic efforts of Miss Frank E. Buttolph (1850-1924), who, in 1900, began to collect menus on the Library's behalf. Miss Buttolph added more than 25,000 menus to the collection, before leaving the Library in 1924. The collection has continued to grow through additional gifts of graphic, gastronomic, topical, or sociological interest, especially but not exclusively New York-related. The collection now contains approximately 45,000 items, about quarter of which have so far been digitized and made available in NTPL Digital Gallery. 
The New York Public Library is digitizing and transcribing its collections of historical menu. The collection includes about 45,000 menus from the 1840s to the present, and the goal of the digitization project is to transcribe each page of each menu, creating an enormous database of dishes, prices, locations, and so on. As of early November 2016, the transcribed database contains 1,332,279 dishes from 17,545 menus.

2.2   Structure
This dataset is split into four files to minimize the amount of redundant information contained in each (and thus, the size of each file). The four data files are Menu, Menu Page, Menu Item, and Dish. These four files are described briefly here, and in detail in their individual file descriptions below.

2.3   Content
   Menu
The core element of the dataset. Each Menu has a unique identifier and associated data, including data on the venue and/or event that the menu was created for; the location that the menu was used; the currency in use on the menu; and various other fields. Each menu is associated with some number of Menu Page values.
   Menu Page
Each Menu Page refers to the Menu it comes from, via the menu_id variable (corresponding to Menu:id). Each Menu Page also has a unique identifier of its own. Associated Menu Page data includes the page number of this Menu Page, an identifier for the scanned image of the page, and the dimensions of the page. Each Menu Page is associated with some number of MenuItem values.
   MenuItem
Each MenuItem refers to both the MenuPage it is found on -- via the menu_page_id variable -- and the Dish that it represents -- via the dish_id variable. Each MenuItem also has a unique identifier of its own. Other associated data includes the price of the item and the dates when the item was created or modified in the database.
   Dish
A Dish is a broad category that covers some number of Menu Items. Each dish has a unique id, to which it is referred by its affiliated MenuItems. Each dish also has a name, a description, several menus it appears on, and both date and price ranges.

2.4   Initial Data quality
From the initial screening, we had a look at the 4 files and following were the observation:
Dish: - The "description" column seemed to be entirely empty, we looked at some first 5000 records. The "menus_appeared" and "times_appeared" seemed to have reported the data properly with the min value at-least equal to 1 which made sense. The "first_appeared" column seemed to have some unusual values like 1 and there were some dishes with lowest and highest price=0.

Menu: -The "name", "keyword", "language", "location_type" columns is entirely empty, for index between 12583-12698 no data was reported. The "sponsor”, "venue", "places" and "event" had missing values at some places. The "occasion" column had a lot of missing information and at many places it reported occasion as other. The "note" column had values that didn’t make much sense at many places. The "currency" and "currency_symbol" also had a lot of data missing.
Menu Item: -"price" column had many missing information, "high price" column had mostly empty data.
Menu Page: -The data was missing for some columns, uuid had data encoded in hexadecimal format which did not prove to be of much use. There was duplicated data.
The dataset brings with it a bunch of information which must be both cleaned and its validity had to be determined. The data was totally un-normalized and could have violated lot of data constraints.

2.5   Use cases that can be derived from the dataset.
•	Data about the dishes can be used to visualize the changing food trends.
•	Identify the effect of the price on the dish popularity.
•	Popular dishes of all time.
•	Biggest sponsor of dishes.
•	Were the dishes being used more for commercial purpose or domestic purpose.
•	On what occasion did the dishes arrived
•	Which menu page go the maximum dish popularity.

The dataset opens a bunch of opportunity to be studied and I am sure by doing some EDA we could have achieve a much broader set of use-cases.

3    Data cleaning with open Refine
After doing initial screening of the data and identifying possible use cases, we thought of using open refine to perform the cleaning of data. Open Refine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; and extending it with web services and external data.
The following were the cleaning actions performed on each file:

Menu
•	Checked if “id” column shows only unique value, the operation was performed by sorting the data and then using the blank only option of open refine which showed 0 rows affected, confirming the understanding.
•	Trim whitespaces from “name” column 9 rows affected.
•	To title case on “name” column 629 cells affected.
•	Merged redundant names from name column, 961 cells affected.
-------Screenshot--------------
•	Title case on “sponsor” column,8683 cells affected.
•	Trim spaces on “sponsor” ,14 cells affected.
•	Cluster values “sponsor” ,3814 cells affected.
•	Removed event, venue, place, physical_description, occasion, keyword, language, location_type column due to missing information all blank cells.
•	Applied regular expression on “call_number” to return on the number and remove the “_wotm” text.
•	-------Screenshot--------------

4    Developing a Relational database schema
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5    Conclusion
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
6    References
[1] https://www.kaggle.com/nypl/whats-on-the-menu/home.
________________________________________________________________________________________________________________________________________________


